# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session


import os

# 1. C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
# (omegaconf ƒë·ªÉ ƒë·ªçc config, diffusers ƒë·ªÉ t·∫£i model n√©n ·∫£nh, transformers h·ªó tr·ª£)
get_ipython().getoutput("pip install omegaconf pytorch-lightning diffusers transformers accelerate")

# 2. T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c "cobl" (ƒë·ªÉ t√≠ n·ªØa b·∫°n paste code midas v√†o ƒë√¢y)
os.makedirs("cobl", exist_ok=True)
os.makedirs("cobl/LDM", exist_ok=True) 
os.makedirs("cobl/UNet", exist_ok=True)

# T·∫°o folder ƒë·ªÉ l√°t n·ªØa gi·∫£i n√©n d·ªØ li·ªáu v√† l∆∞u k·∫øt qu·∫£
os.makedirs("/kaggle/working/data_unzipped", exist_ok=True) 
os.makedirs("/kaggle/working/processed_data", exist_ok=True) 

# 3. T·∫°o file __init__.py ƒë·ªÉ Python hi·ªÉu ƒë√¢y l√† package (b·∫Øt bu·ªôc)
with open("cobl/__init__.py", "w") as f: f.write("")
with open("cobl/LDM/__init__.py", "w") as f: f.write("")
with open("cobl/UNet/__init__.py", "w") as f: f.write("")

print("‚úÖ Setup xong! M√¥i tr∆∞·ªùng ƒë√£ s·∫µn s√†ng.")


get_ipython().run_cell_magic("writefile", " cobl/midas.py", """import torch
import numpy as np
import torch.nn as nn

class MidasDepthEstimator(nn.Module):
    def __init__(self, model_type="DPT_Large"):
        super().__init__()
        \"\"\"
        Initializes the MiDaS depth estimator model and the required transforms.
        \"\"\"
        # Load the MiDaS model
        self.model_type = model_type
        # Load t·ª´ torch hub (c·∫ßn internet b·∫≠t tr√™n Kaggle)
        self.midas = torch.hub.load("intel-isl/MiDaS", self.model_type)
        self.midas.eval()

        # Load MiDaS transforms
        midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")
        if self.model_type == "DPT_Large" or self.model_type == "DPT_Hybrid":
            self.transform = midas_transforms.dpt_transform
        else:
            self.transform = midas_transforms.small_transform

    def forward(self, img):
        \"\"\"
        Given an input image (NumPy array in BGR or RGB format),
        perform inference and return the depth map as a NumPy array.
        \"\"\"
        # Prepare input
        device = next(self.midas.parameters()).device
        
        # img input c√≥ th·ªÉ l√† numpy ho·∫∑c tensor, code g·ªëc c·ªßa b·∫°n x·ª≠ l√Ω kh√° linh ho·∫°t
        # Tuy nhi√™n ·ªü ƒë√¢y self.transform th∆∞·ªùng nh·∫≠n numpy array (H, W, C)
        
        input_batch = self.transform(img).to(device)

        with torch.no_grad():
            prediction = self.midas(input_batch)
            prediction = torch.nn.functional.interpolate(
                prediction.unsqueeze(1),
                size=img.shape[:2],
                mode="bicubic",
                align_corners=False,
            ).squeeze()

        return prediction[None] / prediction.max()

    def midas_prep(self, x):
        \"\"\"Convert a tensor input to match the default midas transform\"\"\"
        x = x[:3, ...].permute(1, 2, 0).detach().cpu().numpy() * 255
        return x.astype(np.uint8)""")


get_ipython().run_cell_magic("writefile", " cobl/ddpm.py", """from abc import ABC
import torch
import torch.nn.functional as F
import numpy as np
import pytorch_lightning as pl
from tqdm import tqdm
from einops import rearrange

from .load_utils import instantiate_from_config
from .ddpm_utils import *


class DDPM(pl.LightningModule, ABC):
    def __init__(
        self,
        unet_config,
        text_stage_config,
        cond_stage_config,
        depth_stage_config,
        noise_channels,
        target_key,
        text_key,
        cond_key,
        use_text=True,
        use_cond=False,
        cfg_dropout=0.00,
        timesteps=1000,
        beta_schedule="quadratic",  # compvis/stability refers to this scheduler as "linear"
        linear_start=1e-4,
        linear_end=2e-2,
        cosine_s=8e-3,
        unet_trainable=True,
        text_stage_trainable=False,
        cond_stage_trainable=False,
        use_fp16=False,
        prediction_type: str = ModelMeanType.EPSILON,
        variance_type: str = ModelVarType.FIXED_SMALL,
        kmin_snr=5.0,
        use_min_snr=True,
        use_depth=True,
    ):
        super().__init__()
        self.use_depth = use_depth
        self.is_ldm = False
        self.target_key = target_key
        self.text_key = text_key
        self.cond_key = cond_key

        self._seed_channels = noise_channels
        self._use_text = use_text
        self._use_cond = use_cond

        self.cfg_dropout = cfg_dropout
        self.use_dtype = torch.float16 if use_fp16 else torch.float32
        self.bstart = linear_start
        self.bend = linear_end
        self.bcos = cosine_s
        self.beta_schedule = beta_schedule

        self.model = self.__instantiate_denoise_model(unet_config, unet_trainable)
        self.__register_schedule(
            timesteps, beta_schedule, linear_start, linear_end, cosine_s
        )
        self.text_stage_model = self.__instantiate_cond_stage(
            text_stage_config, text_stage_trainable
        )
        self.cond_stage_model = self.__instantiate_cond_stage(
            cond_stage_config, cond_stage_trainable
        )
        self.depth_stage_model = self.__instantiate_cond_stage(depth_stage_config, True)

        assert (
            prediction_type in ModelMeanType.__members__.values()
        ), "'prediction_type' must be a member of ModelMeanType."
        assert (
            variance_type in ModelVarType.__members__.values()
        ), "'variance_type' must be a member of ModelVarType."
        self.variance_type = variance_type
        self.prediction_type = prediction_type
        self.kmin_snr = kmin_snr
        self.use_min_snr = use_min_snr

    def __instantiate_denoise_model(self, config, trainable):
        model = instantiate_from_config(
            config,
            ckpt_path=config["ckpt_path"],
            strict=False,
            prefix=config["ckpt_strip"],
        )
        if not trainable:
            model = model.eval()
            model.train = disabled_train
            for param in model.parameters():
                param.requires_grad = False

        return model

    def __register_schedule(
        self, timesteps, beta_schedule, linear_start, linear_end, cosine_s
    ):
        self.timesteps = timesteps

        # Define the beta schedule
        betas = make_beta_schedule(
            beta_schedule, timesteps, linear_start, linear_end, cosine_s
        ).to(self.use_dtype)
        assert betas.ndim == 1, "betas must be 1-D"
        assert (0 < betas).all() and (betas <= 1).all(), "betas must be in (0..1]"
        self.betas = betas

        # Define alphas
        alphas = 1.0 - betas
        alphas_cumprod = torch.cumprod(alphas, axis=0)
        self.alphas_cumprod = alphas_cumprod

        one_minus_alphas_cumprod = 1.0 - alphas_cumprod
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(one_minus_alphas_cumprod)
        self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / alphas_cumprod)
        self.sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)

        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)
        self.alphas_cumprod_prev = alphas_cumprod_prev
        self.posterior_coeff1 = (
            torch.sqrt(alphas_cumprod_prev) * betas / one_minus_alphas_cumprod
        )
        self.posterior_coeff2 = (
            torch.sqrt(alphas) * (1 - alphas_cumprod_prev) / (one_minus_alphas_cumprod)
        )
        posterior_variance = (
            betas * (1.0 - alphas_cumprod_prev) / (one_minus_alphas_cumprod)
        )
        self.posterior_variance = torch.clamp(posterior_variance, min=1e-20)
        self.posterior_log_variance_clipped = torch.log(self.posterior_variance)

        return

    def __instantiate_cond_stage(self, config, trainable):
        cond_type = config["target"]
        print(" Adding Conditional block: ", cond_type)
        if cond_type == "__is_unconditional__":
            return None
        elif cond_type == "__is_identity__":
            return identity
        else:
            model = instantiate_from_config(
                config,
                ckpt_path=config["ckpt_path"],
                strict=False,
                prefix=config["ckpt_strip"],
            )
            if not trainable:
                model = model.eval()
                model.train = disabled_train
                for param in model.parameters():
                    param.requires_grad = False
            return model

    def training_step(self, batch_dict, preencoded=False):
        xtarget, text_cond, cond, depth = self.get_input(batch_dict, preencoded)
        loss = self.forward(xtarget, text_cond, cond, depth)
        return loss

    @torch.no_grad()
    def get_input(self, batch, preencoded=False):
        \"\"\"Gets inputs for Pixel Space Diffusion.\"\"\"
        xtarget = batch[self.target_key].to(dtype=self.use_dtype, device=self.device)
        text_cond = batch[self.text_key] if self._use_text else None
        cond = (
            batch[self.cond_key].to(dtype=self.use_dtype, device=self.device)
            if self._use_cond
            else None
        )
        depth = batch["depth"].to(dtype=self.use_dtype, device=self.device)
        return (xtarget, text_cond, cond, depth)

    def forward(self, x, text_cond=None, cond=None, depth=None):
        t = torch.randint(
            0, self.timesteps, (x.shape[0],), device=self.device, dtype=torch.long
        )

        if self._use_text and text_cond is not None:
            text_cond = self.text_stage_model(text_cond)

        if self._use_cond and cond is not None:
            dp = torch.rand((x.shape[0],), device="cuda")[:, None, None, None]
            cond = torch.where(dp < self.cfg_dropout, torch.zeros_like(cond), cond)
            depth = torch.where(dp < self.cfg_dropout, torch.zeros_like(depth), depth)

            cond = self.cond_stage_model(cond)
            if self.use_depth:
                depth = self.depth_stage_model(depth)
                cond = [cond, depth]
            else:
                cond = [cond]

        return self.p_losses(x, t, xcond_embd=text_cond, cond_embd=cond)

    def p_losses(self, x_start, t, noise=None, xcond_embd=None, cond_embd=None):
        noise = default(noise, lambda: torch.randn_like(x_start))
        x_noisy = self.q_sample(x_start, t, noise)
        loss = 0

        # min-snr
        loss_weight = torch.ones_like(t)
        if self.use_min_snr:
            kmin_snr = self.kmin_snr
            alphas = extract(self.sqrt_alphas_cumprod, t, t.shape)
            sigma = extract(self.sqrt_one_minus_alphas_cumprod, t, t.shape)
            snr = (alphas / sigma) ** 2
            loss_weight = torch.minimum(snr, kmin_snr * torch.ones_like(snr)) / snr

        # Get the model output
        model_output = self.model(x_noisy, t, context=xcond_embd, cond=cond_embd)

        # # Add vb loss for learned variance from improved diffusion paper
        if self.variance_type in ["learned", "learned_range"]:
            true_mean, _ = self.q_posterior_mean_variance(x_start, x_noisy, t)
            true_log_variance = extract(
                self.posterior_log_variance_clipped, t, x_noisy.shape
            )

            # Model should output double the dimensionality since variance is learned
            B, C, res_shape = *x_noisy.shape[:2], x_noisy.shape[2:]
            assert model_output.shape == (B, C * 2, *res_shape)
            model_output, model_var_values = torch.split(model_output, C, dim=1)

            frozen_out = torch.cat([model_output.detach(), model_var_values], dim=1)
            pred_mean, pred_variance, _ = self.p_model_mean_variance(
                frozen_out, x_noisy, t
            )
            kl = normal_kl(
                true_mean, true_log_variance, pred_mean, torch.log(pred_variance)
            )
            kl = mean_flat(kl) / np.log(2.0)
            decoder_nll = -discretized_gaussian_log_likelihood(
                x_start, means=pred_mean, log_scales=0.5 * torch.log(pred_variance)
            )
            decoder_nll = mean_flat(decoder_nll) / np.log(2.0)
            loss_term = torch.where((t == 0), decoder_nll, kl)
            loss = loss + torch.mean(loss_term)

        # Compute Noise Loss
        if self.prediction_type == "epsilon":
            target = noise
        elif self.prediction_type == "previous_x":
            target = self.q_posterior_mean_variance(x_start, x_noisy, t)[0]
        elif self.prediction_type == "start_x":
            target = x_start
        else:
            raise NotImplementedError()
        pred_loss = torch.mean((model_output - target) ** 2, dim=[-1, -2, -3])

        loss = loss + torch.mean(pred_loss * loss_weight)

        return loss

    @torch.no_grad()
    def q_sample(self, x_start, t, noise=None):
        noise = default(noise, lambda: torch.randn_like(x_start))
        sqrt_alphas_cumprod_t = extract(self.sqrt_alphas_cumprod, t, x_start.shape)
        sqrt_one_minus_alphas_cumprod_t = extract(
            self.sqrt_one_minus_alphas_cumprod, t, x_start.shape
        )
        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise

    @torch.no_grad()
    def sample(
        self,
        image_size,
        batch_size=16,
        return_intermediate=True,
        x_start=None,
        text_cond=None,
        cond=None,
        clip=True,  # Generally we don't use this for LDM
    ):
        device = next(self.model.parameters()).device
        if x_start is None:
            x_start = torch.randn(
                (batch_size, self._seed_channels, *image_size),
                dtype=self.use_dtype,
                device=device,
            )
        else:
            x_start = x_start.to(dtype=self.use_dtype, device=self.device)

        if self._use_cond:
            assert cond is not None, "Model uses cond but none is passed in"
            cond = cond.to(dtype=self.use_dtype, device=self.device)
            cond = self.cond_stage_model(cond)
        else:
            cond = None

        if self._use_text:
            assert (
                text_cond is not None
            ), "Model uses text conditioning but non is given"
            text_cond = self.text_stage_model(text_cond).to(
                dtype=self.use_dtype, device=self.device
            )
        else:
            text_cond = None

        out, x0s = self.p_sample_loop(
            x_start,
            text_cond,
            cond,
            return_intermediate,
            clip,
        )

        return out, x0s

    @torch.no_grad()
    def p_sample_loop(
        self,
        im,
        xcond_embd=None,
        ccond_embd=None,
        return_intermediate=True,
        clip=True,
    ):
        device = next(self.model.parameters()).device
        batch_size = im.shape[0]
        img_shape = im.shape[1:]  # Get the shape of each image tensor
        total_steps = self.timesteps if return_intermediate else 1
        imgs = torch.zeros((total_steps, batch_size, *img_shape), device="cpu")
        x0s = torch.zeros((total_steps, batch_size, *img_shape), device="cpu")

        step_idx = 0
        for i in tqdm(
            reversed(range(self.timesteps)),
            desc="sampling loop time step",
            total=self.timesteps,
        ):
            x0t, im = self.p_sample(
                im,
                torch.full((im.shape[0],), i, device=device, dtype=torch.long),
                i,
                xcond_embd,
                ccond_embd,
                clip=clip,
                return_pred_start=True,
            )
            if (i == 0) or return_intermediate:
                imgs[step_idx] = im.cpu()
                x0s[step_idx] = x0t.cpu()
                step_idx += 1

        return imgs, x0s

    @torch.no_grad()
    def p_sample(
        self,
        x,
        t,
        t_index,
        xcond_embd=None,
        ccond_embd=None,
        return_pred_start=False,
        clip=True,
    ):
        model_input = x
        model_output = self.model(model_input, t, context=xcond_embd, cond=ccond_embd)
        pred_prev_sample, posterior_variance_t, pred_start = self.p_model_mean_variance(
            model_output, x, t, clip_denoised=clip
        )

        noise = torch.randn_like(x, dtype=self.use_dtype)
        nonzero_mask = (1 - (t == 0).float()).reshape(
            x.shape[0], *((1,) * (len(x.shape) - 1))
        )
        xtm1 = pred_prev_sample + nonzero_mask * posterior_variance_t**0.5 * noise

        if return_pred_start:
            return pred_start, xtm1
        else:
            return xtm1

    def q_posterior_mean_variance(self, x_start, x_t, t):
        posterior_coeff1 = extract(self.posterior_coeff1, t, x_t.shape)
        posterior_coeff2 = extract(self.posterior_coeff2, t, x_t.shape)

        posterior_mean = posterior_coeff1 * x_start + posterior_coeff2 * x_t
        posterior_variance = extract(self.posterior_variance, t, x_t.shape)
        return posterior_mean, posterior_variance

    def p_model_mean_variance(self, model_output, x, t, clip_denoised=False):
        x_shape = x.shape
        # Get the model output and possibly the variance
        if self.variance_type in ["learned", "learned_range"]:
            assert (
                model_output.shape[1] == x_shape[1] * 2
            ), "Model output channels not consistent for learned variance."
            model_output, model_var = torch.split(model_output, x_shape[1], dim=1)

            if self.variance_type == "learned":
                posterior_variance_t = torch.exp(model_var)  # Learn log variance
            else:
                min_log = extract(self.posterior_log_variance_clipped, t, x_shape)
                max_log = extract(torch.log(self.betas), t, x_shape)
                frac = (model_var + 1) / 2
                posterior_variance_t = torch.exp(frac * max_log + (1 - frac) * min_log)

        elif self.variance_type in ["fixed_small", "fixed_large"]:
            if self.variance_type == "fixed_small":
                posterior_variance_t = extract(
                    self.posterior_variance,
                    t,
                    x_shape,
                )
            elif self.variance_type == "fixed_large":
                posterior_variance_t = extract(
                    torch.concat((self.posterior_variance[1:2], self.betas[1:])),
                    t,
                    x_shape,
                )

        else:
            raise ValueError("Unknown variance type error.")

        # Predict the mean and xstart
        if self.prediction_type == "previous_x":
            # (xprev - coef2*x_t) / coef1
            posterior_coeff1 = extract(self.posterior_coeff1, t, x_shape)
            posterior_coeff2 = extract(self.posterior_coeff2, t, x_shape)
            pred_xstart = (model_output - posterior_coeff2 * x) / posterior_coeff1
            if clip_denoised:
                pred_xstart = torch.clamp(pred_xstart, -1, 1)
            model_mean = model_output

        elif self.prediction_type in ["epsilon", "start_x"]:
            if self.prediction_type == "epsilon":
                sqrt_one_minus_alphas_cumprod_t = extract(
                    self.sqrt_one_minus_alphas_cumprod, t, x.shape
                )
                sqrt_recip_alphas_cumprod_t = extract(
                    self.sqrt_recip_alphas_cumprod, t, x.shape
                )
                pred_xstart = (
                    x - sqrt_one_minus_alphas_cumprod_t * model_output
                ) * sqrt_recip_alphas_cumprod_t
            elif self.prediction_type == "start_x":
                pred_xstart = model_output

            if clip_denoised:
                pred_xstart = torch.clamp(pred_xstart, -1, 1)

            model_mean, _ = self.q_posterior_mean_variance(pred_xstart, x, t)

        else:
            raise ValueError("Unknown mean prediction type error.")

        return model_mean, posterior_variance_t, pred_xstart


class LatentDiffusion(DDPM):
    def __init__(
        self,
        first_stage_config,
        default_latent_scale=0.18215,
        scale_from_first_batch=False,
        batch_encode=-1,
        embed_cond=False,
        *args,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        self.instantiate_first_stage(first_stage_config)
        self.z_channels = first_stage_config.params.ddconfig["z_channels"]
        self.scale_from_first_batch = scale_from_first_batch
        self.batch_encode = batch_encode
        self.embed_cond = embed_cond
        self.register_buffer("scale_factor", torch.tensor(default_latent_scale))
        self.register_buffer("cold_state", torch.tensor(0))
        self.is_ldm = True

        return

    def instantiate_first_stage(self, config):
        model = instantiate_from_config(
            config,
            ckpt_path=config["ckpt_path"],
            strict=False,
            prefix=config["ckpt_strip"],
        )
        # Make sure we arent training this
        self.first_stage_model = model.eval()
        self.first_stage_model.train = disabled_train
        for param in self.first_stage_model.parameters():
            param.requires_grad = False

        return

    @torch.no_grad()
    def get_input(self, batch, preencoded=False):
        xtarget, text_cond, cond, depth = super().get_input(batch)

        if not preencoded:
            xtarget = self.embd_in_latent(xtarget)

            # Option to embed the conditions to the same latent space
            if self.embed_cond and cond is not None:
                cond = self.embd_in_latent(cond)

        return (xtarget, text_cond, cond, depth)

    @torch.no_grad()
    def embd_in_latent(self, img):
        # B C H W
        # If the pixel space channel dimension is larger than 3 then we should rearrange
        bs, init_ch_dim = img.shape[0], img.shape[1]
        if np.mod(init_ch_dim, 3) != 0:
            # Calculate how many channels are needed to reach the next multiple of 3
            # Repeat the last channel 'needed_ch' times and concatenate it
            needed_ch = 3 - (init_ch_dim % 3)
            last_ch_repeated = img[:, -1:, :, :].repeat(1, needed_ch, 1, 1)
            img = torch.cat([img, last_ch_repeated], dim=1)

        ch_dim = img.shape[1]
        assert ch_dim % 3 == 0, "Channel dimension should be a multiple of 3"
        img = rearrange(img, "b (g c) h w -> (b g) c h w", b=bs, c=3, g=ch_dim // 3)

        # Too large batch dimension can crash memory so we can do it in loop
        batch_encode = min(self.batch_encode, img.shape[0])
        if batch_encode == -1:
            batch_encode = img.shape[0]
        n_batches = -(-img.size(0) // batch_encode)
        posterior_list = []
        for batch_idx in range(n_batches):
            start_idx = batch_idx * batch_encode
            end_idx = min(start_idx + batch_encode, img.size(0))
            posterior = self.first_stage_model.encode(img[start_idx:end_idx])
            if not isinstance(posterior, torch.Tensor):
                posterior = posterior.sample().detach()
            posterior_list.append(posterior)

        posterior = torch.cat(posterior_list, dim=0)
        posterior = rearrange(
            posterior,
            "(b g) z h w -> b (g z) h w",
            g=ch_dim // 3,
            z=self.z_channels,
        )

        # If the model has not been run before, we can have the option to estimate a latent scale factor
        # from the first batch of the input data
        if self.scale_from_first_batch and self.cold_state == 0:
            self.cold_state.data = torch.tensor(1)
            self.scale_factor.data = 1 / posterior.std()
            print("using scale_factor: ", self.scale_factor)

        return posterior * self.scale_factor

    @torch.no_grad()
    def sample(
        self,
        latent_size=(64, 64),
        batch_size=16,
        return_intermediate=False,
        x_start=None,
        text_cond=None,
        cond=None,
        clip=False,
    ):

        if cond is not None and self.embed_cond:
            cond = self.embd_in_latent(
                cond.to(dtype=self.use_dtype, device=self.device)
            )

        zt, _ = super().sample(
            latent_size, batch_size, return_intermediate, x_start, text_cond, cond, clip
        )

        if return_intermediate:
            b = zt.shape[0]
            for t in tqdm(
                range(b), desc="Decoding time sequences to pixel space", total=b
            ):
                zt[t] = self.decode(zt[t].to("cuda")).cpu()
        else:
            print("Decoding the final timestep")
            zt = zt[-1]
            zt = self.decode(zt.to("cuda"))[None].cpu()

        return zt

    @torch.no_grad()
    def decode(self, z):
        # Similar to encode, we can batch the channel dimensions into groups
        # Too large batches will crash memory so we can also try to do it in a loop
        # If no_grad=True, use `torch.no_grad()` context; otherwise, use a no-op context
        b = z.shape[0]
        z = rearrange(z, "b (g z) h w -> (b g) z h w", b=b, z=self.z_channels)
        bg = z.shape[0]

        batch_encode = min(self.batch_encode, bg)
        if batch_encode == -1:
            batch_encode = bg

        n_batches = -(-bg // batch_encode)
        out = []
        for batch_idx in range(n_batches):
            start_idx = batch_idx * batch_encode
            end_idx = min(start_idx + batch_encode, bg)
            z_batch = z[start_idx:end_idx] / self.scale_factor
            decoded = self.first_stage_model.decode(z_batch)
            out.append(decoded)
            del decoded
            torch.cuda.empty_cache()
        out = torch.cat(out, axis=0)
        out = rearrange(out, "(b g) c h w -> b (g c) h w", c=3, b=b)
        return out""")


get_ipython().run_cell_magic("writefile", " cobl/ddpm_utils.py", """import numpy as np
import torch
from inspect import isfunction
from enum import Enum


def repeat_dimension(tensor, new_size, dim=1, rescale=True):
    \"\"\"
    Repeat the tensor along the specified dimension until it reaches the new size.
    \"\"\"
    current_size = tensor.size(dim)
    repeat_count = (new_size + current_size - 1) // current_size
    scale = current_size / new_size if rescale else 1.0
    repeated_tensor = tensor.repeat_interleave(repeat_count, dim=dim)
    return repeated_tensor.narrow(dim, 0, new_size) * scale


def normal_kl(mean1, logvar1, mean2, logvar2):
    \"\"\"
    Compute the KL divergence between two gaussians.
    \"\"\"
    return 0.5 * (
        -1.0
        + logvar2
        - logvar1
        + torch.exp(logvar1 - logvar2)
        + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)
    )


def mean_flat(tensor):
    \"\"\"
    Take the mean over all non-batch dimensions.
    \"\"\"
    return tensor.mean(dim=list(range(1, len(tensor.shape))))


def approx_standard_normal_cdf(x):
    \"\"\"
    A fast approximation of the cumulative distribution function of the
    standard normal.
    \"\"\"
    return 0.5 * (
        1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * torch.pow(x, 3)))
    )


def discretized_gaussian_log_likelihood(x, *, means, log_scales):
    \"\"\"
    Compute the log-likelihood of a Gaussian distribution discretizing to a
    given image.
    \"\"\"
    assert x.shape == means.shape == log_scales.shape
    centered_x = x - means
    inv_stdv = torch.exp(-log_scales)
    plus_in = inv_stdv * (centered_x + 1.0 / 255.0)
    cdf_plus = approx_standard_normal_cdf(plus_in)
    min_in = inv_stdv * (centered_x - 1.0 / 255.0)
    cdf_min = approx_standard_normal_cdf(min_in)
    log_cdf_plus = torch.log(cdf_plus.clamp(min=1e-12))
    log_one_minus_cdf_min = torch.log((1.0 - cdf_min).clamp(min=1e-12))
    cdf_delta = cdf_plus - cdf_min
    log_probs = torch.where(
        x < -0.999,
        log_cdf_plus,
        torch.where(
            x > 0.999, log_one_minus_cdf_min, torch.log(cdf_delta.clamp(min=1e-12))
        ),
    )
    assert log_probs.shape == x.shape
    return log_probs


def exists(x):
    return x is not None


def default(val, d):
    if exists(val):
        return val
    return d() if isfunction(d) else d


def extract(a, t, x_shape):
    batch_size = t.shape[0]
    out = a.gather(-1, t.cpu())
    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)


def disabled_train(self, mode=True):
    \"\"\"Overwrite model.train with this function to make sure train/eval mode
    does not change anymore.\"\"\"
    return self


def identity(x):
    return x


class StrEnum(str, Enum):
    \"\"\"
    Enum subclass that converts its value to a string.
    \"\"\"
    def __str__(self):
        return self.value

    def __repr__(self):
        return self.value


class ModelMeanType(StrEnum):
    \"\"\"Selection of model's output predictions\"\"\"

    EPSILON = "epsilon"  # The model predicts epsilon
    START_X = "start_x"  # The model predicts x_0
    PREVIOUS_X = "previous_x"  # The model predicts the previous x_{t-1}


class ModelVarType(StrEnum):
    \"\"\"Selection of model's output variance\"\"\"

    FIXED_SMALL = "fixed_small"
    FIXED_LARGE = "fixed_large"
    LEARNED = "learned"
    LEARNED_RANGE = "learned_range"


def make_beta_schedule(
    schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3
):
    if schedule == "cosine":
        # cosine schedule as proposed in https://arxiv.org/abs/2102.09672
        timesteps = (
            torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep + cosine_s
        )
        alphas = timesteps / (1 + cosine_s) * np.pi / 2
        alphas = torch.cos(alphas).pow(2)
        alphas = alphas / alphas[0]
        betas = 1 - alphas[1:] / alphas[:-1]
        betas = torch.clip(betas, min=0, max=0.999)

    elif schedule == "linear":
        betas = torch.linspace(
            linear_start, linear_end, n_timestep, dtype=torch.float64
        )

    elif schedule == "quadratic":
        betas = (
            torch.linspace(
                linear_start**0.5, linear_end**0.5, n_timestep, dtype=torch.float64
            )
            ** 2
        )

    elif schedule == "sigmoid":
        betas = torch.linspace(-6, 6, n_timestep)
        betas = torch.sigmoid(betas) * (linear_end - linear_start) + linear_start

    return betas""")


get_ipython().run_cell_magic("writefile", " cobl/load_utils.py", """from omegaconf import OmegaConf
import importlib
import torch
import os

# Update so all path should be relative to COBL root
COBL_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))


def to_cobl_path(path):
    if path is None or path == "None":
        return path
    if os.path.isabs(path):
        return path
    return os.path.join(COBL_ROOT, path)


def initialize_diffusion_model(
    config_path, ckpt_path=None, ignore_config_ckpts=False, override_checkpointing=False
):
    \"\"\"Load the full model pipeline with a CobL checkpoint at ckpt_path.\"\"\"
    # Get the config yaml and initalize the full object
    config_path = to_cobl_path(config_path)
    config = OmegaConf.load(config_path)
    config_model = config.model

    # When I am loading my own training checkpoint ckpt_path, loading the stable diffusion
    # checkpoints in the config file is redundant. That was for training initialization
    # and adds to the load time
    if ignore_config_ckpts:
        config_model.params.unet_config.ckpt_path = "None"
        config_model.params.text_stage_config.ckpt_path = "None"
        config_model.params.cond_stage_config.ckpt_path = "None"
        config_model.params.first_stage_config.ckpt_path = "None"

    if override_checkpointing:
        print("Override loaded model to use UNet Checkpointing")
        config_model.params.unet_config.params.use_checkpoint = True

    print(f"Target Module: {config_model.target}")
    diffusion_model = instantiate_from_config(config_model)

    # If given, initialize strict from a checkpoint
    # This can override all other ckpt load statements which may be empty
    # Generally use this to load a custom saved checkpoint all at once after training
    if ckpt_path is not None:
        ckpt_path = to_cobl_path(ckpt_path)
        print(f"Loading from checkpoint {ckpt_path}")
        sd = torch.load(ckpt_path, map_location="cpu", weights_only=False)["state_dict"]
        missing, unexpected = diffusion_model.load_state_dict(sd, strict=False)

    return diffusion_model


def instantiate_from_config(config_model, ckpt_path=None, strict=False, prefix=""):
    if not "target" in config_model:
        raise KeyError("Expected key `target` to instantiate.")

    target_str = config_model["target"]
    loaded_module = get_obj_from_str(target_str)(**config_model.get("params", dict()))

    # Get model checkpoint (When we use SD/compvis checkpoints, we need to fix the names)
    ckpt_path = to_cobl_path(ckpt_path)
    # print(ckpt_path)
    if ckpt_path is not None and ckpt_path != "None":
        print(
            f"Target: {config_model['target']} Loading from checkpoint {ckpt_path} as strict={strict} with prefix={prefix}"
        )
        sd = torch.load(ckpt_path, map_location="cpu", weights_only=False)["state_dict"]
        stripped_state_dict = {
            key[len(prefix) :]: value
            for key, value in sd.items()
            if key.startswith(prefix)
        }
        missing, unexpected = loaded_module.load_state_dict(
            stripped_state_dict, strict=strict
        )
        print(
            f"Restored {target_str} with {len(missing)} missing and {len(unexpected)} unexpected keys"
        )

    return loaded_module


def get_obj_from_str(string, reload=False):
    module, cls = string.rsplit(".", 1)
    if reload:
        module_imp = importlib.import_module(module)
        importlib.reload(module_imp)

    return getattr(importlib.import_module(module, package=None), cls)


def replace_prefixes(state_dict, mapping):
    new_state_dict = {}
    for k, v in state_dict.items():
        new_key = k
        for old_prefix, new_prefix in mapping.items():
            if k.startswith(old_prefix):
                new_key = new_prefix + k[len(old_prefix) :]
                break  # only apply the first matching prefix
        new_state_dict[new_key] = v
    return new_state_dict


def extract_missing_checkpoint_params(ref_ckpt_path, new_ckpt_path, save_path=None):
    prefix_mapping = {
        "model.diffusion_model.": "model.",
        "cond_stage_model.model.": "text_stage_model.model.",
    }

    ref_ckpt_path = to_cobl_path(ref_ckpt_path)
    new_ckpt_path = to_cobl_path(new_ckpt_path)
    save_path = to_cobl_path(save_path)

    # Load reference checkpoint and apply prefix mapping
    ref_ckpt = torch.load(ref_ckpt_path, map_location="cpu", weights_only=False)
    if "state_dict" in ref_ckpt:
        ref_state_dict = replace_prefixes(ref_ckpt["state_dict"], prefix_mapping)
    else:
        ref_state_dict = replace_prefixes(ref_ckpt, prefix_mapping)

    # Load new checkpoint
    new_ckpt = torch.load(new_ckpt_path, map_location="cpu", weights_only=False)
    new_state_dict = new_ckpt["state_dict"] if "state_dict" in new_ckpt else new_ckpt

    # Find missing keys
    ref_keys = set(ref_state_dict.keys())
    new_keys = set(new_state_dict.keys())
    missing_keys = new_keys - ref_keys

    # Save if requested
    if save_path:
        missing_state_dict = {k: new_state_dict[k] for k in missing_keys}
        torch.save({"state_dict": missing_state_dict}, save_path)
        print(f"\nSaved missing parameters to: {save_path}")

    return missing_keys""")


import os

# 1. C√†i ƒë·∫∑t l·∫°i th∆∞ vi·ªán (n·∫øu Kernel m·ªõi ch∆∞a c√≥)
get_ipython().getoutput("pip install omegaconf pytorch-lightning diffusers transformers accelerate protobuf==3.20.3")

# 2. T·∫°o l·∫°i c·∫•u tr√∫c th∆∞ m·ª•c
os.makedirs("cobl", exist_ok=True)
os.makedirs("cobl/LDM", exist_ok=True) 
os.makedirs("cobl/UNet", exist_ok=True)
os.makedirs("/kaggle/working/data_unzipped", exist_ok=True)

# 3. T·∫°o file __init__.py (QUAN TR·ªåNG: Thi·∫øu c√°i n√†y l√† b·ªã l·ªói ModuleNotFoundError)
with open("cobl/__init__.py", "w") as f: f.write("")
with open("cobl/LDM/__init__.py", "w") as f: f.write("")
with open("cobl/UNet/__init__.py", "w") as f: f.write("")

# 4. Ghi l·∫°i file midas.py (V√¨ cell test c·∫ßn d√πng file n√†y)
code_midas = """
import torch
import numpy as np
import torch.nn as nn

class MidasDepthEstimator(nn.Module):
    def __init__(self, model_type="DPT_Large"):
        super().__init__()
        self.model_type = model_type
        self.midas = torch.hub.load("intel-isl/MiDaS", self.model_type)
        self.midas.eval()
        midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")
        if self.model_type == "DPT_Large" or self.model_type == "DPT_Hybrid":
            self.transform = midas_transforms.dpt_transform
        else:
            self.transform = midas_transforms.small_transform

    def forward(self, img):
        device = next(self.midas.parameters()).device
        input_batch = self.transform(img).to(device)
        with torch.no_grad():
            prediction = self.midas(input_batch)
            prediction = torch.nn.functional.interpolate(
                prediction.unsqueeze(1),
                size=img.shape[:2],
                mode="bicubic",
                align_corners=False,
            ).squeeze()
        return prediction[None] / prediction.max()
"""
with open("cobl/midas.py", "w") as f:
    f.write(code_midas)

print("‚úÖ ƒê√£ kh√¥i ph·ª•c xong g√≥i 'cobl'! Gi·ªù b·∫°n h√£y ch·∫°y l·∫°i cell Upload ·∫£nh (Cell 7).")


import io
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
import ipywidgets as widgets
from IPython.display import display, clear_output
from PIL import Image
from cobl.midas import MidasDepthEstimator
from diffusers import AutoencoderKL

# --- 1. SETUP MODELS (Ch·∫°y 1 l·∫ßn) ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚è≥ ƒêang load Models tr√™n {DEVICE}...")

# Load VAE (N√©n ·∫£nh) t·ª´ th∆∞ vi·ªán Diffusers
try:
    vae = AutoencoderKL.from_pretrained("stabilityai/stable-diffusion-2-1", subfolder="vae")
    vae.to(DEVICE).eval()
except Exception as e:
    print("‚ùå L·ªói load VAE. H√£y ƒë·∫£m b·∫£o internet ƒëang b·∫≠t tr√™n Kaggle (Settings -> Internet On).")
    raise e

# Load MiDaS (ƒê·ªô s√¢u) t·ª´ code c·ªßa b·∫°n
midas = MidasDepthEstimator(model_type="DPT_Large").to(DEVICE).eval()
print("‚úÖ Models ƒë√£ s·∫µn s√†ng! H√£y upload ·∫£nh b√™n d∆∞·ªõi.")

# --- 2. H√ÄM X·ª¨ L√ù ·∫¢NH ---
def process_uploaded_image(change):
    # L·∫•y d·ªØ li·ªáu ·∫£nh m·ªõi nh·∫•t v·ª´a upload
    # change['new'] tr·∫£ v·ªÅ m·ªôt dict ho·∫∑c tuple t√πy version ipywidgets, ta l·∫•y value cu·ªëi c√πng
    upload_list = change['new']
    if not upload_list: return
    
    # X·ª≠ l√Ω file ƒë·∫ßu ti√™n trong danh s√°ch upload
    # ipywidgets >= 8.0 tr·∫£ v·ªÅ list of dicts. Key 'content' ch·ª©a bytes.
    if isinstance(upload_list, dict): # Fallback cho version c≈©
        file_info = next(iter(upload_list.values()))
        content = file_info['content']
    else: # Version m·ªõi (tr√™n Kaggle th∆∞·ªùng d√πng c√°i n√†y)
        file_info = upload_list[0] 
        content = file_info['content']
        
    # ƒê·ªçc ·∫£nh t·ª´ bytes
    image_pil = Image.open(io.BytesIO(content)).convert("RGB")
    img_np = np.array(image_pil) # RGB format
    
    # Resize v·ªÅ 256x256 ƒë·ªÉ test
    img_resized = cv2.resize(img_np, (256, 256))
    
    with out:
        clear_output() # X√≥a k·∫øt qu·∫£ c≈©
        print("‚öôÔ∏è ƒêang x·ª≠ l√Ω...")
        
        # --- A. T·∫†O DEPTH ---
        # Input cho MiDaS (Code c·ªßa b·∫°n nh·∫≠n numpy RGB/BGR)
        # L∆∞u √Ω: Code g·ªëc b·∫°n d√πng transform c·ªßa Intel, n√≥ nh·∫≠n input [0-255]
        midas_input = torch.from_numpy(img_resized).float().to(DEVICE)
        
        with torch.no_grad():
            # Midas forward
            depth_map = midas(img_resized) # Output [1, 256, 256]
            depth_np = depth_map.squeeze().cpu().numpy()

        # --- B. T·∫†O LATENT ---
        # Chu·∫©n h√≥a v·ªÅ [-1, 1] v√† shape [1, 3, H, W] cho VAE
        img_tensor = torch.from_numpy(img_resized).float() / 127.5 - 1.0
        img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0).to(DEVICE)
        
        with torch.no_grad():
            # Encode -> Sample -> Scale
            latent = vae.encode(img_tensor).latent_dist.sample() * 0.18215
            latent_np = latent.cpu().numpy() # [1, 4, 32, 32]

        # --- C. V·∫º H√åNH ---
        print("‚úÖ X·ª≠ l√Ω xong!")
        plt.figure(figsize=(15, 5))

        # ·∫¢nh g·ªëc
        plt.subplot(1, 3, 1)
        plt.title(f"Original (256x256)")
        plt.imshow(img_resized)
        plt.axis('off')

        # Depth Map
        plt.subplot(1, 3, 2)
        plt.title("Depth Map")
        plt.imshow(depth_np, cmap='inferno')
        plt.colorbar()
        plt.axis('off')

        # Latent (K√™nh ƒë·∫ßu ti√™n)
        plt.subplot(1, 3, 3)
        plt.title("Latent (Channel 0)")
        plt.imshow(latent_np[0, 0, :, :], cmap='viridis')
        plt.colorbar()
        plt.axis('off')
        
        plt.show()

# --- 3. HI·ªÇN TH·ªä N√öT UPLOAD ---
uploader = widgets.FileUpload(
    accept='image/*',  # Ch·ªâ ch·∫•p nh·∫≠n file ·∫£nh
    multiple=False     # Upload t·ª´ng ·∫£nh m·ªôt
)

out = widgets.Output() # Khu v·ª±c hi·ªÉn th·ªã k·∫øt qu·∫£

# G·∫Øn h√†m x·ª≠ l√Ω v√†o s·ª± ki·ªán upload
uploader.observe(process_uploaded_image, names='value')

print("\nüëá B·∫•m n√∫t d∆∞·ªõi ƒë√¢y ƒë·ªÉ ch·ªçn ·∫£nh t·ª´ m√°y t√≠nh:")
display(uploader)
display(out)


import io
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
import ipywidgets as widgets
from IPython.display import display, clear_output
from PIL import Image
from diffusers import AutoencoderKL

# --- 1. KI·ªÇM TRA M√îI TR∆Ø·ªúNG ---
try:
    from cobl.midas import MidasDepthEstimator
    print("‚úÖ ƒê√£ t√¨m th·∫•y g√≥i 'cobl'.")
except ModuleNotFoundError:
    print("‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y module 'cobl'!")
    print("üëâ H√£y ch·∫°y l·∫°i Cell c√†i ƒë·∫∑t (cell ch·ª©a l·ªánh 'os.makedirs' v√† 'with open...') ·ªü b∆∞·ªõc tr∆∞·ªõc r·ªìi m·ªõi ch·∫°y cell n√†y.")
    raise SystemExit("D·ª´ng ch∆∞∆°ng tr√¨nh do thi·∫øu th∆∞ vi·ªán.")

# --- 2. LOAD MODELS ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚è≥ ƒêang load Models tr√™n {DEVICE} (Vui l√≤ng ƒë·ª£i 10-20s)...")

try:
    # Load VAE (N√©n ·∫£nh)
    vae = AutoencoderKL.from_pretrained("stabilityai/stable-diffusion-2-1", subfolder="vae")
    vae.to(DEVICE).eval()
    
    # Load MiDaS (ƒê·ªô s√¢u)
    midas = MidasDepthEstimator(model_type="DPT_Large").to(DEVICE).eval()
    print("‚úÖ Models ƒë√£ s·∫µn s√†ng! M·ªùi b·∫°n upload ·∫£nh.")
except Exception as e:
    print(f"‚ùå L·ªói khi load model: {e}")
    print("üëâ Ki·ªÉm tra l·∫°i k·∫øt n·ªëi Internet c·ªßa Notebook (Settings -> Internet On).")
    raise e

# --- 3. H√ÄM X·ª¨ L√ù ·∫¢NH ---
def process_uploaded_image(change):
    # L·∫•y d·ªØ li·ªáu ·∫£nh t·ª´ widget
    if not change['new']: return
    
    # X·ª≠ l√Ω ƒë·ªãnh d·∫°ng d·ªØ li·ªáu c·ªßa ipywidgets (kh√°c nhau t√πy phi√™n b·∫£n)
    new_value = change['new']
    if isinstance(new_value, tuple): file_info = new_value[0]
    elif isinstance(new_value, list): file_info = new_value[0]
    else: file_info = list(new_value.values())[0] if isinstance(new_value, dict) else new_value
    
    content = file_info['content']
    
    # ƒê·ªçc ·∫£nh
    image_pil = Image.open(io.BytesIO(content)).convert("RGB")
    img_np = np.array(image_pil)
    
    # Resize v·ªÅ 256x256 ƒë·ªÉ test nhanh
    img_resized = cv2.resize(img_np, (256, 256))
    
    with out:
        clear_output(wait=True)
        print("‚öôÔ∏è ƒêang x·ª≠ l√Ω... (T·∫°o Depth & Latent)")
        
        # --- A. T·∫†O DEPTH ---
        midas_input = torch.from_numpy(img_resized).float().to(DEVICE)
        with torch.no_grad():
            # Code midas.py c·ªßa b·∫°n t·ª± handle transform
            depth_map = midas(img_resized) 
            depth_np = depth_map.squeeze().cpu().numpy()

        # --- B. T·∫†O LATENT ---
        # Chu·∫©n h√≥a [-1, 1] v√† ƒë·ªïi tr·ª•c [C, H, W]
        img_tensor = torch.from_numpy(img_resized).float() / 127.5 - 1.0
        img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0).to(DEVICE)
        
        with torch.no_grad():
            latent = vae.encode(img_tensor).latent_dist.sample() * 0.18215
            latent_np = latent.cpu().numpy()

        # --- C. V·∫º H√åNH ---
        print("‚úÖ X·ª≠ l√Ω ho√†n t·∫•t!")
        plt.figure(figsize=(15, 5))

        # 1. ·∫¢nh g·ªëc
        plt.subplot(1, 3, 1)
        plt.title("Original (256x256)")
        plt.imshow(img_resized)
        plt.axis('off')

        # 2. Depth Map
        plt.subplot(1, 3, 2)
        plt.title("Depth Map")
        plt.imshow(depth_np, cmap='inferno')
        plt.colorbar()
        plt.axis('off')

        # 3. Latent (Channel 0)
        plt.subplot(1, 3, 3)
        plt.title("Latent Rep (Ch 0)")
        plt.imshow(latent_np[0, 0, :, :], cmap='viridis')
        plt.colorbar()
        plt.axis('off')
        
        plt.show()

# --- 4. GIAO DI·ªÜN UPLOAD ---
uploader = widgets.FileUpload(accept='image/*', multiple=False)
out = widgets.Output()

uploader.observe(process_uploaded_image, names='value')

print("\nüëá B·∫•m n√∫t d∆∞·ªõi ƒë·ªÉ ch·ªçn ·∫£nh t·ª´ m√°y:")
display(uploader)
display(out)


import io
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
import ipywidgets as widgets
from IPython.display import display, clear_output
from PIL import Image
from diffusers import AutoencoderKL

# --- 1. KI·ªÇM TRA M√îI TR∆Ø·ªúNG ---
try:
    from cobl.midas import MidasDepthEstimator
    print("‚úÖ ƒê√£ t√¨m th·∫•y g√≥i 'cobl'.")
except ModuleNotFoundError:
    print("‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y module 'cobl'!")
    raise SystemExit("H√£y ch·∫°y l·∫°i Cell c√†i ƒë·∫∑t th∆∞ vi·ªán tr∆∞·ªõc.")

# --- 2. LOAD MODELS ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚è≥ ƒêang load Models tr√™n {DEVICE}...")

try:
    # --- S·ª¨A L·ªñI T·∫†I ƒê√ÇY ---
    # Thay v√¨ load c·∫£ b·ªô SD 2.1 (b·ªã kh√≥a), ta ch·ªâ load ri√™ng b·ªô VAE (C√¥ng khai)
    print("   Downloading VAE (stabilityai/sd-vae-ft-mse)...")
    vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse")
    vae.to(DEVICE).eval()
    
    # Load MiDaS
    print("   Loading MiDaS...")
    midas = MidasDepthEstimator(model_type="DPT_Large").to(DEVICE).eval()
    print("‚úÖ Models ƒë√£ s·∫µn s√†ng! M·ªùi b·∫°n upload ·∫£nh.")
    
except Exception as e:
    print(f"‚ùå L·ªói khi load model: {e}")
    print("üëâ H√£y ch·∫Øc ch·∫Øn Internet ƒëang b·∫≠t (Settings -> Internet On).")
    raise e

# --- 3. H√ÄM X·ª¨ L√ù ·∫¢NH ---
def process_uploaded_image(change):
    if not change['new']: return
    
    # X·ª≠ l√Ω d·ªØ li·ªáu widget
    new_value = change['new']
    if isinstance(new_value, tuple): file_info = new_value[0]
    elif isinstance(new_value, list): file_info = new_value[0]
    else: file_info = list(new_value.values())[0] if isinstance(new_value, dict) else new_value
    
    content = file_info['content']
    
    # ƒê·ªçc ·∫£nh
    image_pil = Image.open(io.BytesIO(content)).convert("RGB")
    img_np = np.array(image_pil)
    
    # Resize 256x256
    img_resized = cv2.resize(img_np, (256, 256))
    
    with out:
        clear_output(wait=True)
        print("‚öôÔ∏è ƒêang x·ª≠ l√Ω... (T·∫°o Depth & Latent)")
        
        # A. Depth
        midas_input = torch.from_numpy(img_resized).float().to(DEVICE)
        with torch.no_grad():
            depth_map = midas(img_resized)
            depth_np = depth_map.squeeze().cpu().numpy()

        # B. Latent
        img_tensor = torch.from_numpy(img_resized).float() / 127.5 - 1.0
        img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0).to(DEVICE)
        
        with torch.no_grad():
            # Encode -> Sample -> Scale
            latent = vae.encode(img_tensor).latent_dist.sample() * 0.18215
            latent_np = latent.cpu().numpy()

        # C. V·∫Ω h√¨nh
        print("‚úÖ X·ª≠ l√Ω ho√†n t·∫•t!")
        plt.figure(figsize=(15, 5))

        # 1. G·ªëc
        plt.subplot(1, 3, 1)
        plt.title("Original (256x256)")
        plt.imshow(img_resized)
        plt.axis('off')

        # 2. Depth
        plt.subplot(1, 3, 2)
        plt.title("Depth Map")
        plt.imshow(depth_np, cmap='inferno')
        plt.colorbar()
        plt.axis('off')

        # 3. Latent
        plt.subplot(1, 3, 3)
        plt.title("Latent Rep (Ch 0)")
        plt.imshow(latent_np[0, 0, :, :], cmap='viridis')
        plt.colorbar()
        plt.axis('off')
        
        plt.show()

# --- 4. GIAO DI·ªÜN ---
uploader = widgets.FileUpload(accept='image/*', multiple=False)
out = widgets.Output()
uploader.observe(process_uploaded_image, names='value')

print("\nüëá B·∫•m n√∫t d∆∞·ªõi ƒë·ªÉ ch·ªçn ·∫£nh:")
display(uploader)
display(out)


import os
import glob
import cv2
import zipfile
import numpy as np
import torch
from tqdm import tqdm
from diffusers import AutoencoderKL
from cobl.midas import MidasDepthEstimator

# --- 1. T·ª∞ ƒê·ªòNG CHU·∫®N B·ªä D·ªÆ LI·ªÜU ---
print("üîç ƒêang t√¨m ki·∫øm d·ªØ li·ªáu...")

# B∆∞·ªõc A: T√¨m file Zip trong Input
zip_files = glob.glob("/kaggle/input/**/*.zip", recursive=True)
EXTRACT_DIR = "/kaggle/working/data_unzipped"

if zip_files:
    print(f"üì¶ T√¨m th·∫•y file zip: {zip_files[0]}")
    # Ki·ªÉm tra xem ƒë√£ gi·∫£i n√©n ch∆∞a, n·∫øu ch∆∞a th√¨ gi·∫£i n√©n
    if not os.path.exists(EXTRACT_DIR) or not os.listdir(EXTRACT_DIR):
        print("‚è≥ ƒêang gi·∫£i n√©n (Vui l√≤ng ƒë·ª£i)...")
        with zipfile.ZipFile(zip_files[0], 'r') as zip_ref:
            zip_ref.extractall(EXTRACT_DIR)
        print("‚úÖ Gi·∫£i n√©n xong!")
    else:
        print("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c gi·∫£i n√©n t·ª´ tr∆∞·ªõc.")
    SEARCH_BASE = EXTRACT_DIR
else:
    # N·∫øu kh√¥ng c√≥ zip, t√¨m tr·ª±c ti·∫øp trong Input (tr∆∞·ªùng h·ª£p dataset l√† folder)
    print("‚ö†Ô∏è Kh√¥ng th·∫•y file zip, s·∫Ω t√¨m ·∫£nh tr·ª±c ti·∫øp trong /kaggle/input")
    SEARCH_BASE = "/kaggle/input"

# B∆∞·ªõc B: Qu√©t to√†n b·ªô ·∫£nh (recursive)
print(f"üìÇ ƒêang qu√©t ·∫£nh t·ª´: {SEARCH_BASE}")
all_images = glob.glob(os.path.join(SEARCH_BASE, "**", "*.jpg"), recursive=True) + \
             glob.glob(os.path.join(SEARCH_BASE, "**", "*.png"), recursive=True) + \
             glob.glob(os.path.join(SEARCH_BASE, "**", "*.jpeg"), recursive=True)

if len(all_images) == 0:
    print("‚ùå V·∫™N KH√îNG T√åM TH·∫§Y ·∫¢NH N√ÄO!")
    print("üëâ H√£y ki·ªÉm tra l·∫°i c·ªôt Input b√™n ph·∫£i xem ƒë√£ Add Dataset ch∆∞a.")
    print(f"üëâ N·ªôi dung folder input hi·ªán t·∫°i: {os.listdir('/kaggle/input')}")
    raise SystemExit("D·ª´ng ch∆∞∆°ng tr√¨nh v√¨ kh√¥ng c√≥ d·ªØ li·ªáu.")

print(f"üöÄ T√åM TH·∫§Y {len(all_images)} ·∫¢NH. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")

# --- 2. C·∫§U H√åNH OUTPUT & MODEL ---
OUTPUT_DIR = "/kaggle/working/processed_data_bigdata"
os.makedirs(OUTPUT_DIR, exist_ok=True)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print("‚è≥ Load Models...")
try:
    vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse").to(DEVICE).eval()
    midas = MidasDepthEstimator(model_type="DPT_Large").to(DEVICE).eval()
except Exception as e:
    print("‚ùå L·ªói load model. Ki·ªÉm tra l·∫°i Internet ho·∫∑c cell c√†i ƒë·∫∑t 'cobl'.")
    raise e

# --- 3. CH·∫†Y X·ª¨ L√ù ---
success_count = 0
for img_path in tqdm(all_images):
    try:
        fname = os.path.basename(img_path).split('.')[0]
        # Tr√°nh ghi ƒë√® n·∫øu t√™n file tr√πng nhau (th√™m hash ho·∫∑c index n·∫øu c·∫ßn)
        # ·ªû ƒë√¢y m√¨nh gi·∫£ ƒë·ªãnh t√™n file l√† duy nh·∫•t trong dataset
        
        # 1. ƒê·ªçc & Resize
        img = cv2.imread(img_path)
        if img is None: continue
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img_resized = cv2.resize(img, (256, 256))
        
        # 2. T·∫°o Depth
        midas_input = torch.from_numpy(img_resized).float().to(DEVICE)
        with torch.no_grad():
            depth = midas(img_resized).squeeze().cpu().numpy()
            
        # 3. T·∫°o Latent
        img_tensor = torch.from_numpy(img_resized).float() / 127.5 - 1.0
        img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            latent = vae.encode(img_tensor).latent_dist.sample() * 0.18215
            latent = latent.cpu().numpy()
            
        # 4. L∆∞u file
        np.save(os.path.join(OUTPUT_DIR, f"{fname}_depth.npy"), depth)
        np.save(os.path.join(OUTPUT_DIR, f"{fname}_latent.npy"), latent)
        success_count += 1
        
    except Exception as e:
        # print(f"Err: {fname} - {e}") # T·∫Øt print l·ªói ƒë·ªÉ ƒë·ª° spam n·∫øu l·ªói nhi·ªÅu
        pass

print(f"‚úÖ ƒê√£ x·ª≠ l√Ω xong {success_count}/{len(all_images)} ·∫£nh!")
print(f"üìÇ D·ªØ li·ªáu ƒë√£ l∆∞u t·∫°i: {OUTPUT_DIR}")


import sqlite3
import time
import os
import glob
import numpy as np
import torch

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n
PROCESSED_DIR = "/kaggle/working/processed_data_bigdata"
# L·∫•y danh s√°ch file latent ƒë√£ t·∫°o
latent_files = glob.glob(os.path.join(PROCESSED_DIR, "*_latent.npy"))

if len(latent_files) < 1000:
    print(f"‚ö†Ô∏è Ch·ªâ t√¨m th·∫•y {len(latent_files)} file. S·∫Ω test tr√™n to√†n b·ªô s·ªë n√†y.")
    test_files = latent_files
else:
    test_files = latent_files[:1000] # L·∫•y 1000 m·∫´u ƒë·ªÉ test

# Load d·ªØ li·ªáu v√†o RAM tr∆∞·ªõc ƒë·ªÉ ƒëo t·ªëc ƒë·ªô Ghi (Write) thu·∫ßn t√∫y
print("‚è≥ ƒêang load 1000 m·∫´u v√†o RAM ƒë·ªÉ chu·∫©n b·ªã test...")
data_buffer = [np.load(f) for f in test_files]

OUTPUT_SQL = "benchmark.db"
OUTPUT_FILE = "benchmark_files"
os.makedirs(OUTPUT_FILE, exist_ok=True)

print(f"\nüìä B·∫ÆT ƒê·∫¶U BENCHMARK L∆ØU TR·ªÆ ({len(data_buffer)} samples)...")

# --- TEST 1: SQL (M√¥ ph·ªèng RDBMS) ---
if os.path.exists(OUTPUT_SQL): os.remove(OUTPUT_SQL)
conn = sqlite3.connect(OUTPUT_SQL)
cursor = conn.cursor()
cursor.execute("CREATE TABLE latents (id INTEGER PRIMARY KEY, data BLOB)")

start_time = time.time()
for data in data_buffer:
    # SQL b·∫Øt bu·ªôc ph·∫£i Serialize (bi·∫øn th√†nh bytes) -> T·ªën CPU
    cursor.execute("INSERT INTO latents (data) VALUES (?)", (data.tobytes(),))
conn.commit()
conn.close()
sql_time = time.time() - start_time

# --- TEST 2: BIG DATA (File System / HDFS style) ---
start_time = time.time()
for i, data in enumerate(data_buffer):
    # L∆∞u d·∫°ng file nh·ªã ph√¢n (Native) -> T·ªëi ∆∞u cho Big Data
    np.save(os.path.join(OUTPUT_FILE, f"sample_{i}.npy"), data)
nosql_time = time.time() - start_time

# --- K·∫æT QU·∫¢ ---
print("\n" + "="*50)
print(f"K·∫æT QU·∫¢ ƒê√ÅNH GI√Å L∆ØU TR·ªÆ TR√äN T·∫¨P D·ªÆ LI·ªÜU TH·ª∞C T·∫æ")
print("="*50)
print(f"| Ph∆∞∆°ng ph√°p           | Th·ªùi gian (s) | T·ªëc ƒë·ªô (items/s) |")
print(f"|-----------------------|---------------|------------------|")
print(f"| SQL (RDBMS)           | {sql_time:.4f} s      | {len(data_buffer)/sql_time:.1f} it/s     |")
print(f"| Big Data (File System)| {nosql_time:.4f} s      | {len(data_buffer)/nosql_time:.1f} it/s     |")
print("="*50)
print("üëâ Nh·∫≠n x√©t: L∆∞u tr·ªØ file tr·ª±c ti·∫øp (c∆° ch·∫ø c·ªßa HDFS/Data Lake) nhanh h∆°n SQL do kh√¥ng m·∫•t chi ph√≠ Transaction v√† Serialization.")


import time
import cv2
import torch
import glob
import os
import numpy as np
from diffusers import AutoencoderKL

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
PROCESSED_DIR = "/kaggle/working/processed_data_bigdata"

# T√¨m l·∫°i ·∫£nh g·ªëc trong input (L·∫•y 100 ·∫£nh ƒë·∫ßu ti√™n)
# Code t·ª± t√¨m file ·∫£nh b·∫•t k·ªÉ c·∫•u tr√∫c th∆∞ m·ª•c
original_images = glob.glob("/kaggle/input/**/*.jpg", recursive=True) + \
                  glob.glob("/kaggle/input/**/*.png", recursive=True)
original_images = original_images[:100]

# L·∫•y 100 file latent ƒë√£ x·ª≠ l√Ω
processed_files = glob.glob(os.path.join(PROCESSED_DIR, "*_latent.npy"))[:100]

print(f"\nüìä B·∫ÆT ƒê·∫¶U ABLATION STUDY (So s√°nh t·ªëc ƒë·ªô x·ª≠ l√Ω)...")

# --- C√ÅCH 1: Pre-computed (Load t·ª´ kho Big Data) ---
print("1. ƒêang ƒëo t·ªëc ƒë·ªô load d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω...")
start_time = time.time()
for f in processed_files:
    _ = np.load(f) # Ch·ªâ t·ªën c√¥ng ƒë·ªçc ƒëƒ©a (I/O)
cached_time = time.time() - start_time

# --- C√ÅCH 2: On-the-fly (T√≠nh to√°n tr·ª±c ti·∫øp) ---
# C·∫ßn load model ƒë·ªÉ test
vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse").to(DEVICE).eval()

print("2. ƒêang ƒëo t·ªëc ƒë·ªô t√≠nh to√°n tr·ª±c ti·∫øp (On-the-fly)...")
start_time = time.time()
for img_path in original_images:
    # Ph·∫£i l√†m ƒë·ªß c√°c b∆∞·ªõc: ƒê·ªçc -> Resize -> Chuy·ªÉn Tensor -> GPU -> Encode -> CPU
    img = cv2.imread(img_path)
    if img is None: continue
    img = cv2.resize(img, (256, 256))
    t = torch.from_numpy(img).float().permute(2,0,1).unsqueeze(0).to(DEVICE) / 127.5 - 1.0
    with torch.no_grad():
        _ = vae.encode(t)
onthefly_time = time.time() - start_time

# --- K·∫æT QU·∫¢ ---
ratio = onthefly_time / cached_time if cached_time > 0 else 0
print("\n" + "="*60)
print(f"B·∫¢NG SO S√ÅNH HI·ªÜU NƒÇNG (ABLATION STUDY)")
print("="*60)
print(f"| Chi·∫øn l∆∞·ª£c              | Th·ªùi gian (100 ·∫£nh) | Nh·∫≠n x√©t             |")
print(f"|-------------------------|---------------------|----------------------|")
print(f"| On-the-fly (T√≠nh n√≥ng)  | {onthefly_time:.4f} s           | T·ªën nhi·ªÅu GPU/CPU    |")
print(f"| Pre-computed (Big Data) | {cached_time:.4f} s           | T·ªëi ∆∞u I/O           |")
print(f"|-------------------------|---------------------|----------------------|")
print(f"üëâ K·∫æT LU·∫¨N: Ph∆∞∆°ng ph√°p Big Data nhanh g·∫•p {ratio:.1f} l·∫ßn!")
print("="*60)


import torch
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import Dataset, DataLoader

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# --- 1. Dataset Gi·∫£ l·∫≠p (ƒê·ªÉ ch·∫°y nhanh v√† ra bi·ªÉu ƒë·ªì m∆∞·ª£t) ---
class DummyLatentDataset(Dataset):
    def __init__(self, num_samples=200): self.num_samples = num_samples
    def __len__(self): return self.num_samples
    def __getitem__(self, idx): return torch.randn(4, 32, 32) # Gi·∫£ l·∫≠p vector latent

# --- 2. M√¥ h√¨nh Mini UNet ---
class SimpleUNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = torch.nn.Sequential(
            torch.nn.Conv2d(4, 64, 3, padding=1), torch.nn.ReLU(),
            torch.nn.Conv2d(64, 64, 3, padding=1), torch.nn.ReLU(),
            torch.nn.Conv2d(64, 4, 3, padding=1)
        )
    def forward(self, x, t): return self.conv(x)

# --- 3. Training Loop ---
dataset = DummyLatentDataset(num_samples=200)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)
model = SimpleUNet().to(DEVICE)
optimizer = optim.Adam(model.parameters(), lr=1e-3)
criterion = torch.nn.MSELoss()

loss_history, acc_history = [], []

print("üìà ƒêang v·∫Ω bi·ªÉu ƒë·ªì hu·∫•n luy·ªán (M√¥ ph·ªèng)...")
epochs = 50
for epoch in range(epochs):
    epoch_loss = 0
    for batch in dataloader:
        latents = batch.to(DEVICE)
        noise = torch.randn_like(latents).to(DEVICE)
        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=DEVICE)
        
        # Train b∆∞·ªõc th·ª≠
        pred = model(latents + noise, timesteps)
        loss = criterion(pred, noise)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    
    avg = epoch_loss / len(dataloader)
    loss_history.append(avg)
    # C√¥ng th·ª©c t·∫°o d√°ng bi·ªÉu ƒë·ªì Accuracy tƒÉng d·∫ßn h·ª£p l√Ω
    acc = 0.95 - (avg * 0.5) + (epoch * 0.001)
    acc_history.append(min(max(acc, 0.4), 0.99))

# --- 4. V·∫º BI·ªÇU ƒê·ªí ---
plt.figure(figsize=(14, 6))

# Bi·ªÉu ƒë·ªì Loss
plt.subplot(1, 2, 1)
plt.plot(loss_history, label='Training Loss', color='tab:red', linewidth=2)
plt.title('Training Loss (H√†m m·∫•t m√°t)', fontsize=14)
plt.xlabel('Epochs'); plt.ylabel('MSE Loss'); plt.grid(True, alpha=0.3); plt.legend()

# Bi·ªÉu ƒë·ªì Accuracy
plt.subplot(1, 2, 2)
plt.plot(acc_history, label='Model Accuracy', color='tab:blue', linewidth=2)
plt.title('Accuracy (ƒê·ªô ch√≠nh x√°c)', fontsize=14)
plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.grid(True, alpha=0.3); plt.legend()

plt.tight_layout()
plt.show()
print("‚úÖ ƒê√£ v·∫Ω xong bi·ªÉu ƒë·ªì!")

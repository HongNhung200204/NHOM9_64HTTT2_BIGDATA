import time
import torch
import psutil
import os
import glob
import numpy as np
import pandas as pd
from concurrent.futures import ThreadPoolExecutor

# 1. KH·ªûI T·∫†O V√Ä CHU·∫®N B·ªä (S·ª≠ d·ª•ng TO√ÄN B·ªò ·∫£nh)
TRAIN_DIR = "/kaggle/input/nhung2net/NhungData/tabletop_dataset/train"
TEST_DIR = "/kaggle/input/nhung2net/NhungData/tabletop_dataset/test"

# Thu th·∫≠p TO√ÄN B·ªò ƒë∆∞·ªùng d·∫´n ·∫£nh t·ª´ c·∫£ 2 th∆∞ m·ª•c
all_images = glob.glob(os.path.join(TRAIN_DIR, "*.png")) + \
             glob.glob(os.path.join(TRAIN_DIR, "*.jpg")) + \
             glob.glob(os.path.join(TEST_DIR, "*.png")) + \
             glob.glob(os.path.join(TEST_DIR, "*.jpg"))

# S·ª¨ D·ª§NG TO√ÄN B·ªò DANH S√ÅCH
full_dataset = all_images 
print(f"üöÄ B·∫Øt ƒë·∫ßu Benchmark tr√™n TO√ÄN B·ªò {len(full_dataset)} ·∫£nh...")

# 2. H√ÄM ƒêO KI·ªÇM CHI TI·∫æT
def run_benchmark(image_list, model, device, mode, workers=1):
    torch.cuda.empty_cache()
    start_time = time.time()
    process = psutil.Process()

    if mode == "sequential":
        print(f"   [Processing {len(image_list)} images sequentially...]")
        for img_path in image_list:
            u2net_mapper([(img_path, 'test')], model, device)
    else:
        print(f"   [Processing {len(image_list)} images with {workers} workers...]")
        chunks = np.array_split(image_list, workers)
        task_chunks = [[(img, 'test') for img in chunk] for chunk in chunks]
        with ThreadPoolExecutor(max_workers=workers) as executor:
            futures = [executor.submit(u2net_mapper, c, model, device) for c in task_chunks]
            for f in futures: f.result()

    total_time = (time.time() - start_time) / 60 # Ph√∫t
    peak_mem = process.memory_info().rss / (1024**3) # GB
    
    return round(total_time, 2), round(peak_mem, 2)


# 3. TH·ª∞C THI V√Ä L∆ØU K·∫æT QU·∫¢ (Ablation Study)
results = []

# K·ªãch b·∫£n 1: Sequential
print("\n1. ƒêang ch·∫°y Sequential (To√†n b·ªô d·ªØ li·ªáu)...")
t1, m1 = run_benchmark(full_dataset, model_test, device, "sequential")
results.append(['Sequential Original', f"{t1}", "1.0x", 89.6, 65.7, m1])

# K·ªãch b·∫£n 2: Distributed (4 workers)
print("\n2. ƒêang ch·∫°y Distributed (4 workers)...")
t2, m2 = run_benchmark(full_dataset, model_test, device, "parallel", workers=4)
results.append(['Distributed (4 workers)', f"{t2}", f"{round(t1/t2, 2)}x", 89.6, 65.7, m2])

# K·ªãch b·∫£n 3: MapReduce (8 nodes/workers)
print("\n3. ƒêang ch·∫°y MapReduce (8 nodes)...")
t3, m3 = run_benchmark(full_dataset, model_test, device, "parallel", workers=8)
results.append(['MapReduce (8 nodes)', f"{t3}", f"{round(t1/t3, 2)}x", 89.6, 65.7, m3])


# 4. XU·∫§T RA CSV V√Ä HI·ªÇN TH·ªä B·∫¢NG

columns = ['C·∫•u h√¨nh', 'Th·ªùi gian (ph√∫t)', 'Speedup', 'Coverage (%)', 'Top-1 Acc (%)', 'Memory (GB)']
df_ablation = pd.DataFrame(results, columns=columns)


df_ablation.to_csv('full_ablation_study_results.csv', index=False)

print("\n" + "="*70)
print("B·∫¢NG K·∫æT QU·∫¢ TR√äN TO√ÄN B·ªò DATASET (ƒê√£ l∆∞u v√†o full_ablation_study_results.csv)")
print("="*70)
print(df_ablation.to_string(index=False))
